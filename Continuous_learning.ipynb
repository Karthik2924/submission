{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZtYK0VPX29k4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "elif torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ],
      "metadata": {
        "id": "pvQa_Iwn3NUc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(64*8*8, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 64*8*8)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return nn.functional.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "H32pFits3hrj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, ewc_lambda, precision_matrices,prior_means):\n",
        "    print(len(train_loader))\n",
        "    model.train()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if target[0]==3 or target[0]==5:\n",
        "            target = torch.where(target == 3, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in precision_matrices:\n",
        "                fisher = precision_matrices[name]\n",
        "                ewc_loss = (fisher * (param - prior_means[name])**2).sum()\n",
        "                loss += ewc_lambda * ewc_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "def train_continuous(model, train_loader, optimizer, ewc_lambda, n_epochs, n_tasks):\n",
        "    model.prev_means = {}\n",
        "    model.prec_matrices = {}\n",
        "    for task in range(n_tasks):\n",
        "        print(\"Task:\", task)\n",
        "        if task == 0:\n",
        "            train(model, train_loader[task], optimizer, ewc_lambda, {},model.prev_means)\n",
        "        else:\n",
        "            print(\"here task number\", task)\n",
        "            train_loader_new = train_loader[task]\n",
        "            train_loader_prev = []\n",
        "            for i in range(task):\n",
        "                train_loader_prev.append(train_loader[i])\n",
        "            precision_matrices = compute_fisher(model, train_loader_prev)\n",
        "            model.prev_means = {}\n",
        "            for name, param in model.named_parameters():\n",
        "                model.prev_means[name] = param.data.clone()\n",
        "            train(model, train_loader_new, optimizer, ewc_lambda, precision_matrices,model.prev_means)\n",
        "            model.prec_matrices = {}\n",
        "            for name, param in model.named_parameters():\n",
        "                if name in precision_matrices:\n",
        "                    model.prec_matrices[name] = precision_matrices[name].clone()\n",
        "                    \n",
        "def compute_fisher(model, data_loader):\n",
        "    data_loader = data_loader[0]\n",
        "    precision_matrices = {}\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                fisher = (param.grad.detach()**2)\n",
        "                if name in precision_matrices:\n",
        "                    precision_matrices[name] += fisher\n",
        "                else:\n",
        "                    precision_matrices[name] = fisher\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in precision_matrices:\n",
        "            precision_matrices[name] /= len(data_loader)\n",
        "    return precision_matrices\n"
      ],
      "metadata": {
        "id": "1UTivuQv9riV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Define the transform to normalize the data\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the entire CIFAR10 dataset\n",
        "full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a subset of the dataset containing only classes 0 and 1\n",
        "indices = torch.where((torch.tensor(full_dataset.targets) == 0) | (torch.tensor(full_dataset.targets) == 1))[0]\n",
        "subset_dataset = Subset(full_dataset, indices)\n",
        "\n",
        "# Create a dataloader for the subset dataset\n",
        "train_loader_task1 = DataLoader(subset_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "indices = torch.where((torch.tensor(test_dataset.targets) == 0) | (torch.tensor(test_dataset.targets) == 1))[0]\n",
        "subset_dataset = Subset(test_dataset, indices)\n",
        "\n",
        "# Create a dataloader for the subset dataset\n",
        "test_loader_task1 = DataLoader(subset_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "indices = torch.where((torch.tensor(full_dataset.targets) == 3) | (torch.tensor(full_dataset.targets) == 5))[0]\n",
        "subset_dataset = Subset(full_dataset, indices)\n",
        "\n",
        "# Create a dataloader for the subset dataset\n",
        "train_loader_task2 = DataLoader(subset_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "indices = torch.where((torch.tensor(test_dataset.targets) == 3) | (torch.tensor(test_dataset.targets) == 5))[0]\n",
        "subset_dataset = Subset(test_dataset, indices)\n",
        "\n",
        "# Create a dataloader for the subset dataset\n",
        "test_loader_task2 = DataLoader(subset_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLtbhpy0IMt1",
        "outputId": "0797bc83-78c8-4542-852b-514d9adbd082"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kjdhw2RYkD0",
        "outputId": "d1dfc2e3-23fb-4960-bef5-c93a7cfb1e5a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_dataset.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "uwjIG7vqYM0s",
        "outputId": "0fcad917-6bc8-4fb4-e472-5fa90e33c24b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-292cd0b9b114>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubset_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'targets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loaders, n_tasks):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for task in range(n_tasks):\n",
        "            total_loss = 0\n",
        "            total_correct = 0\n",
        "            total_samples = 0\n",
        "            for data, target in test_loaders[task]:\n",
        "                if target[0]==3 or target[0]==5:\n",
        "                  target = torch.where(target == 3, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "                output = model(data)\n",
        "                loss = loss_fn(output, target)\n",
        "                total_loss += loss.item() * data.size(0)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total_correct += (predicted == target).sum().item()\n",
        "                total_samples += data.size(0)\n",
        "            print(\"Task:\", task, \"Loss:\", total_loss / total_samples, \"Accuracy:\", total_correct / total_samples)\n"
      ],
      "metadata": {
        "id": "YpNyv_LET6GR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "n_tasks = 2\n",
        "lr = 0.001\n",
        "ewc_lambda = 0.5\n",
        "\n",
        "model = CNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Train on Task 1\n",
        "train_continuous(model, [train_loader_task1, train_loader_task2], optimizer, ewc_lambda, n_epochs, n_tasks)\n",
        "#test(model, [test_loader_task1, test_loader_task2], n_tasks)"
      ],
      "metadata": {
        "id": "5R1aji7-TY7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, [test_loader_task1, test_loader_task2], n_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpu1gF05awrV",
        "outputId": "73b27fcc-91ff-4a6b-8894-79b3191b5673"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: 0 Loss: 0.6429721341133118 Accuracy: 0.6125\n",
            "Task: 1 Loss: 0.6171479921340942 Accuracy: 0.6595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_grKLT6tUkaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}